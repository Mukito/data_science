# Data Science
A ciência de dados é uma área interdisciplinar que combina análise estatística, aprendizado de 
máquina, mineração de dados e tecnologias de big data para extrair insights e conhecimento de 
dados estruturados e não estruturados. Ela envolve uma variedade de técnicas da ciência da 
computação, matemática e conhecimento específico de cada domínio para resolver problemas 
complexos e tomar decisões baseadas em dados.

Os principais componentes da ciência de dados incluem:

### 1. Coleta e Limpeza de Dados: 
Obtenção de dados de várias fontes e garantia de que estejam precisos, consistentes e formatados corretamente para análise.

### 2. Análise Exploratória de Dados (EDA): 
Uso de técnicas estatísticas e de visualização para entender a estrutura e os padrões presentes nos dados.

### 3. Engenharia de Atributos (Feature Engineering): 
Criação de variáveis ou atributos relevantes que podem melhorar o desempenho dos modelos.

### 4. Modelagem e Aprendizado de Máquina: 
Aplicação de algoritmos para construir modelos preditivos ou descritivos. Isso pode incluir aprendizado supervisionado (como regressão, classificação) ou aprendizado não supervisionado (como agrupamento, redução de dimensionalidade).

### 5. Avaliação e Validação: 
Avaliação do desempenho do modelo usando várias métricas e garantindo que o modelo generalize bem para dados não vistos.

### 6. Implantação: 
Integração do modelo em sistemas de produção ou aplicações para fazer previsões ou automatizar decisões.

### 7. Comunicação e Visualização: 
Apresentação dos resultados de maneira compreensível, geralmente por meio de painéis, relatórios ou visualizações interativas.

Ferramentas comuns usadas em ciência de dados incluem linguagens de programação como **Python** e **R**, bibliotecas como **pandas**, **numpy**, **scikit-learn** e **TensorFlow**, além de ferramentas de visualização de dados como **Matplotlib**, **Seaborn** e **Tableau**.

### =================================
Aprender **Data Science** leva tempo, requer dedicação e o aprendizado deve ocorrer com a utilização de material organizado e estruturado, com suporte para as dúvidas comuns no começo da jornada. Mas pode ser muito recompensador trabalhar com Data Science, pois os salários são acima da média, a empregabilidade é alta e há chance de trabalhar em empresas de qualquer setor. 

E como você consegue sua tão sonhada vaga no mercado? Como você demonstra aos recrutadores o conhecimento que vem adquirindo? Montar um portfólio de projetos pode ser um excelente caminho.

Mas qualquer projeto de Data Science tem como matéria-prima, dados. Ok, temos então mais um desafio a ser superado: como obter dados para montar um portfólio de projetos? Aqui está a resposta, neste artigo.

Vamos listar 5 fontes de dados que permitem você fazer o download e usar gratuitamente diversos datasets públicos, que podem ser usados como ponto de partida em seus projetos de Data Science.

A propósito: lembre-se sempre de referenciar suas fontes de dados. Não use dados privados e nem use dados sem a devida autorização. Estamos na era da LGPD (Lei Geral de Proteção de Dados) além de outras leis que regem o uso de dados ao redor do mundo.

Os datasets disponíveis aqui são públicos e podem ser usados livremente, mas certifique-se de checar os termos de uso.

### 1- Scikit-Learn
O Scikit-Learn é o principal framework Python para construção de modelos de aprendizado de máquina e contém várias APIs para diversos conjuntos de dados, desde dados simples, passando por dados reais, até a geração de dados para um propósito específico. Aqui estão os links para você:

Toy Datasets

Real World datasets

Generated Datasets

Other Datasets

### 2- NLTK
NLTK é um pacote Python específico para o trabalho de Processamento de Linguagem Natural. O NLTK também fornece conjuntos de dados de texto que você pode usar para seus projetos.

Existem dezenas de conjuntos de dados de texto do NLTK disponíveis para uso. Consulte a lista completa aqui: NLTK Corpora

### 3- Statsmodels
Statsmodels é um pacote Python para modelagem estatística, mas o pacote também fornece vários conjuntos de dados que podem ser usados em seus projetos. Aqui a lista completa: Statsmodels Datasets

### 4- Pydataset
Pydataset é um pacote Python que fornece vários conjuntos de dados de código aberto. Os datasets são básicos, mas podem ser um bom ponto de partida para um projeto ou para um experimento com uma nova biblioteca de Machine Learning. Confira o pacote aqui: Pydataset

### 5- Datasets
Datasets é um pacote Python da HuggingFace criado especificamente para acessar e compartilhar conjuntos de dados. 

O que é ótimo no pacote datasets é que, não importa o tamanho do conjunto de dados, você pode processar o conjunto de dados com leituras de cópia zero sem nenhuma restrição de memória, pois o pacote datasets usa o Apache Arrow em segundo plano.

Você pode examinar o hub HuggingFace do pacote datasets para obter a lista completa com milhares de conjuntos de dados: Datasets

#### ============================
## Low-Code: <br>
é   uma   abordagem   de   desenvolvimento   de   software   que   minimiza   a 
necessidade de programação manual para criar aplicações e processos. Plataformas Low-Code 
permitem aos usuários desenvolver aplicações através de interfaces gráficas e configurações, ao 
invés de escrita de código extensiva. A ideia é simplificar o processo de desenvolvimento  para 
que  usuários  com  pouca  ou  nenhuma  habilidade  de  programação  possam  criar  aplicações, ao 
mesmo tempo em que aceleram o desenvolvimento para programadores experientes. 

### Características Principais das Plataformas Low-Code:<br>

  • Interfaces Gráficas de Usuário (GUIs): Ferramentas de arrastar e soltar que permitem aos usuários montar aplicações visualmente.<br>
  • Reutilização  de  Componentes:  Capacidade  de  usar  e  reusar  blocos  de  construção predefinidos para funções comuns.<br>
  • Automação  de  Processos:  Ferramentas  para  criar  workflows  automatizados  com pouca necessidade de codificação.<br>
  • Integrações  Fáceis:  Capacidade  de  integrar  com  bancos  de  dados,  APIs  e  outros serviços sem escrever código complexo <br>


### Ferramentas Low-Code
Power BI <br>
Qlik Sense <br>
looker <br>

Machine Learning Low-Code <br>
DataRobot <br>
H2O.ai <br>
Azure Machine Learning Studio <br>
Google Cloud AutoML <br>
AWS SageMaker <br>
Knime <br>
RapidMiner <br>
IBM Watson Studio - (Excelentes Já testada) <br>

Dados em Grandes Escala <br>
Apache Hadoop (elefante amarelo) Gratuito <br>
Apache Spark - (interessante) <br>
Google BigQuery - nuvem Mais fácil <br> 
Amazon Redshift - nuvem Mais poderoso <br>
Snowflake <br>
Microsoft Azure Synapse Analytics (SQL Data Warehouse) <br>
Databricks <br>
Apache Kafka <br>


Data Lakes - é um repositório centralizado que permite armazenar todos os tipos de dados, 
estruturados ou não de forma Bruta. Armazenar grandes volumes de dados em um formato nativo
flexibilidade e escalabilidade

Data Warehouses - Armazena dados já processados e estruturados de varias fontes para 
tomada de decisão e analise de Business Inteligence

Data Lakehouse - Paradigma emergente que combina elementos de Lakes e Warehouse
